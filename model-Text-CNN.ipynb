{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib as jl\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use mnist\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))\n",
    "# use 20newsgroups\n",
    "x_train, y_train, x_val, y_val = jl.load('data.jl.z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainning_framework(x_train, y_train, x_val, y_val, model):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    \n",
    "    n_epoch = 20\n",
    "    batch_size = 12\n",
    "    learning_rate = 0.0001\n",
    "    print_freq = 5\n",
    "    \n",
    "    x, y_, y, cost, acc, net = model()\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "    net.print_params()\n",
    "    net.print_layers()\n",
    "    \n",
    "    train_net(sess, train_op, x, y, y_, cost, acc, net, x_train, y_train, x_val, y_val, \n",
    "              print_freq, batch_size, n_epoch)\n",
    "    \n",
    "\n",
    "    \n",
    "def train_net(sess, train_op, x, y, y_, cost, acc, net, x_train, y_train, x_val, y_val, \n",
    "              print_freq =5, batch_size = 128, n_epoch = 200):\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        for x_train_a, y_train_a in tl.iterate.minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "            feed_dict = {x: x_train_a, y_: y_train_a}\n",
    "            feed_dict.update(net.all_drop)  # enable dropout or dropconnect layers\n",
    "            sess.run(train_op, feed_dict=feed_dict)\n",
    "            \n",
    "        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:        \n",
    "            print(\"Epoch %d of %d took %fs\" % (epoch + 1, n_epoch, time.time() - start_time))\n",
    "            train_loss, train_acc, n_batch = 0, 0, 0\n",
    "            for x_train_a, y_train_a in tl.iterate.minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "                dp_dict = tl.utils.dict_to_one(net.all_drop)  # disable noise layers\n",
    "                feed_dict = {x: x_train_a, y_: y_train_a}\n",
    "                feed_dict.update(dp_dict)\n",
    "                err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "                train_loss += err\n",
    "                train_acc += ac\n",
    "                n_batch += 1\n",
    "            print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "            print(\"   train acc: %f\" % (train_acc/ n_batch))\n",
    "\n",
    "            val_loss, val_acc, n_batch = 0, 0, 0\n",
    "            for x_val_a, y_val_a in tl.iterate.minibatches(x_val, y_val, batch_size, shuffle=True):\n",
    "                dp_dict = tl.utils.dict_to_one(net.all_drop)  # disable noise layers\n",
    "                feed_dict = {x: x_val_a, y_: y_val_a}\n",
    "                feed_dict.update(dp_dict)\n",
    "                err, ac = sess.run([cost, acc], feed_dict=feed_dict)\n",
    "                val_loss += err\n",
    "                val_acc += ac\n",
    "                n_batch += 1\n",
    "            print(\"   val loss: %f\" % (val_loss / n_batch))\n",
    "            print(\"   val acc: %f\" % (val_acc / n_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_model():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
    "    y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n",
    "    \n",
    "    net = tl.layers.InputLayer(x, name='input')\n",
    "    net = tl.layers.DropoutLayer(net, keep=0.8, name='drop1')\n",
    "    net = tl.layers.DenseLayer(net, n_units=800, act=tf.nn.relu, name='relu1')\n",
    "    net = tl.layers.DropoutLayer(net, keep=0.5, name='drop2')\n",
    "    net = tl.layers.DenseLayer(net, n_units=800, act=tf.nn.relu, name='relu2')\n",
    "    net = tl.layers.DropoutLayer(net, keep=0.5, name='drop3')\n",
    "    net = tl.layers.DenseLayer(net, n_units=10, act=tf.identity, name='output')\n",
    "    \n",
    "    y = net.outputs\n",
    "    \n",
    "    cost = tl.cost.cross_entropy(y, y_, name='xentropy')\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return x, y_, y, cost, acc, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
    "    y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n",
    "    \n",
    "    net = tl.layers.InputLayer(x, name='input')\n",
    "    net = tl.layers.ReshapeLayer(net, [-1, 28, 28, 1], name='reshape')\n",
    "    net = tl.layers.Conv2dLayer(net,\n",
    "                        act = tf.nn.relu,\n",
    "                        shape = [5, 5, 1, 32],  # 32 features for each 5x5 patch\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME',\n",
    "                        name ='cnn1')     # output: (?, 28, 28, 32)\n",
    "    net = tl.layers.PoolLayer(net,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME',\n",
    "                        pool = tf.nn.max_pool,\n",
    "                        name ='pool1',)   # output: (?, 14, 14, 32)\n",
    "    net = tl.layers.Conv2dLayer(net,\n",
    "                        act = tf.nn.relu,\n",
    "                        shape = [5, 5, 32, 64], # 64 features for each 5x5 patch\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME',\n",
    "                        name ='cnn2')     # output: (?, 14, 14, 64)\n",
    "    net = tl.layers.PoolLayer(net,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME',\n",
    "                        pool = tf.nn.max_pool,\n",
    "                        name ='pool2',)   # output: (?, 7, 7, 64)\n",
    "    net = tl.layers.FlattenLayer(net, name='flatten')\n",
    "    net = tl.layers.DenseLayer(net, n_units=128, act=tf.identity, name='fc1')\n",
    "        \n",
    "    net = tl.layers.DropoutLayer(net, keep=0.5, name='drop')\n",
    "    net = tl.layers.DenseLayer(net, n_units=10, act=tf.identity, name='output')\n",
    "    \n",
    "    y = net.outputs\n",
    "    \n",
    "    cost = tl.cost.cross_entropy(y, y_, name='xentropy')\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return x, y_, y, cost, acc, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_text_model(vocab_size=30000, hidden_size=64):\n",
    "    \n",
    "    doc_length = 1024\n",
    "    \n",
    "    x = tf.placeholder(tf.int32, shape=(None, doc_length))\n",
    "    y_ = tf.placeholder(tf.int64, shape=[None], name='y_')\n",
    "        \n",
    "    net = tl.layers.EmbeddingInputlayer(x, vocab_size, hidden_size, name='embedding')\n",
    "    net = tl.layers.ReshapeLayer(net, [-1, doc_length, hidden_size, 1], name='reshape')\n",
    "    \n",
    "    cov1 = tl.layers.Conv2dLayer(net,\n",
    "                        act = tf.nn.relu,\n",
    "                        shape = [3, hidden_size, 1, 100],  \n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name ='cnn1')\n",
    "    \n",
    "    pool1 = tl.layers.PoolLayer(cov1,\n",
    "                        ksize=[1, 1022, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        pool = tf.nn.max_pool,\n",
    "                        name ='pool1')\n",
    "    \n",
    "    cov2 = tl.layers.Conv2dLayer(net,\n",
    "                        act = tf.nn.relu,\n",
    "                        shape = [4, hidden_size, 1, 100],  \n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name ='cnn2')\n",
    "    \n",
    "    pool2 = tl.layers.PoolLayer(cov2,\n",
    "                        ksize=[1, 1021, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        pool = tf.nn.max_pool,\n",
    "                        name ='pool2') \n",
    "    \n",
    "    cov3 = tl.layers.Conv2dLayer(net,\n",
    "                        act = tf.nn.relu,\n",
    "                        shape = [5, hidden_size, 1, 100],  # 32 features for each 5x5 patch\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name ='cnn3')\n",
    "    \n",
    "    pool3 = tl.layers.PoolLayer(cov3,\n",
    "                        ksize=[1, 1020, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        pool = tf.nn.max_pool,\n",
    "                        name ='pool3')\n",
    "    \n",
    "    net = tl.layers.ConcatLayer([pool1, pool2, pool3], concat_dim=1, name ='concat_layer')\n",
    "    net = tl.layers.FlattenLayer(net, name='flatten')\n",
    "    net = tl.layers.DenseLayer(net, n_units=128, act=tf.identity, name='fc1')\n",
    "    net = tl.layers.DropoutLayer(net, keep=0.5, name='drop')\n",
    "    net = tl.layers.DenseLayer(net, n_units=20, act=tf.identity, name='output')\n",
    "    \n",
    "    y = net.outputs\n",
    "    \n",
    "    cost = tl.cost.cross_entropy(y, y_, name='xentropy')\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return x, y_, y, cost, acc, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] EmbeddingInputlayer embedding: (30000, 64)\n",
      "[TL] ReshapeLayer reshape: (?, 1024, 64, 1)\n",
      "[TL] Conv2dLayer cnn1: shape:[3, 64, 1, 100] strides:[1, 1, 1, 1] pad:VALID act:relu\n",
      "[TL] PoolLayer   pool1: ksize:[1, 1022, 1, 1] strides:[1, 1, 1, 1] padding:VALID pool:max_pool\n",
      "[TL] Conv2dLayer cnn2: shape:[4, 64, 1, 100] strides:[1, 1, 1, 1] pad:VALID act:relu\n",
      "[TL] PoolLayer   pool2: ksize:[1, 1021, 1, 1] strides:[1, 1, 1, 1] padding:VALID pool:max_pool\n",
      "[TL] Conv2dLayer cnn3: shape:[5, 64, 1, 100] strides:[1, 1, 1, 1] pad:VALID act:relu\n",
      "[TL] PoolLayer   pool3: ksize:[1, 1020, 1, 1] strides:[1, 1, 1, 1] padding:VALID pool:max_pool\n",
      "[TL] ConcatLayer concat_layer: axis: 1\n",
      "[TL] FlattenLayer flatten: 300\n",
      "[TL] DenseLayer  fc1: 128 identity\n",
      "[TL] DropoutLayer drop: keep:0.500000 is_fix:False\n",
      "[TL] DenseLayer  output: 20 identity\n",
      "[TL]   param   0: embedding/embeddings:0 (30000, 64)        float32_ref (mean: 5.6605334975756705e-05, median: 9.73232090473175e-05, std: 0.05771826207637787)   \n",
      "[TL]   param   1: cnn1/W_conv2d:0      (3, 64, 1, 100)    float32_ref (mean: -5.8537087170407176e-05, median: -0.0001621890114620328, std: 0.017514299601316452)   \n",
      "[TL]   param   2: cnn1/b_conv2d:0      (100,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   3: cnn2/W_conv2d:0      (4, 64, 1, 100)    float32_ref (mean: -6.608343392144889e-05, median: -0.00010939608910121024, std: 0.01760151796042919)   \n",
      "[TL]   param   4: cnn2/b_conv2d:0      (100,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   5: cnn3/W_conv2d:0      (5, 64, 1, 100)    float32_ref (mean: -0.00011393501335987821, median: -6.055303674656898e-05, std: 0.0175708569586277)   \n",
      "[TL]   param   6: cnn3/b_conv2d:0      (100,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   7: fc1/W:0              (300, 128)         float32_ref (mean: -0.0004184483550488949, median: -0.0004123356775380671, std: 0.08822517842054367)   \n",
      "[TL]   param   8: fc1/b:0              (128,)             float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   param   9: output/W:0           (128, 20)          float32_ref (mean: -0.0006446106126531959, median: -0.0016707592876628041, std: 0.08706673234701157)   \n",
      "[TL]   param  10: output/b:0           (20,)              float32_ref (mean: 0.0               , median: 0.0               , std: 0.0               )   \n",
      "[TL]   num of params: 2038208\n",
      "[TL]   layer   0: embedding/embedding_lookup:0 (?, 1024, 64)      float32\n",
      "[TL]   layer   1: reshape:0            (?, 1024, 64, 1)    float32\n",
      "[TL]   layer   2: cnn1/Relu:0          (?, 1022, 1, 100)    float32\n",
      "[TL]   layer   3: pool1:0              (?, 1, 1, 100)     float32\n",
      "[TL]   layer   4: cnn2/Relu:0          (?, 1021, 1, 100)    float32\n",
      "[TL]   layer   5: pool2:0              (?, 1, 1, 100)     float32\n",
      "[TL]   layer   6: cnn3/Relu:0          (?, 1020, 1, 100)    float32\n",
      "[TL]   layer   7: pool3:0              (?, 1, 1, 100)     float32\n",
      "[TL]   layer   8: concat_layer:0       (?, 3, 1, 100)     float32\n",
      "[TL]   layer   9: flatten:0            (?, 300)           float32\n",
      "[TL]   layer  10: fc1/Identity:0       (?, 128)           float32\n",
      "[TL]   layer  11: drop/mul:0           (?, 128)           float32\n",
      "[TL]   layer  12: output/Identity:0    (?, 20)            float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20 took 22.886410s\n",
      "   train loss: 2.934517\n",
      "   train acc: 0.165163\n",
      "   val loss: 2.954755\n",
      "   val acc: 0.126263\n",
      "Epoch 5 of 20 took 18.299933s\n",
      "   train loss: 0.856847\n",
      "   train acc: 0.795648\n",
      "   val loss: 1.203148\n",
      "   val acc: 0.659357\n",
      "Epoch 10 of 20 took 18.636708s\n",
      "   train loss: 0.150140\n",
      "   train acc: 0.973461\n",
      "   val loss: 0.749526\n",
      "   val acc: 0.782961\n",
      "Epoch 15 of 20 took 18.670223s\n",
      "   train loss: 0.018517\n",
      "   train acc: 0.998142\n",
      "   val loss: 0.760714\n",
      "   val acc: 0.801568\n",
      "Epoch 20 of 20 took 19.424948s\n",
      "   train loss: 0.003117\n",
      "   train acc: 0.999469\n",
      "   val loss: 0.861047\n",
      "   val acc: 0.803030\n"
     ]
    }
   ],
   "source": [
    "trainning_framework(x_train, y_train, x_val, y_val, cnn_text_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
