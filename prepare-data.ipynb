{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib as jl\n",
    "from scipy.sparse import csc_matrix\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_data(root_path):\n",
    "    train_path = os.path.join(root_path, '20news-bydate-train')\n",
    "    test_path = os.path.join(root_path, '20news-bydate-test')\n",
    "    train_topic = os.listdir(train_path)\n",
    "    test_topic = os.listdir(test_path)\n",
    "    topic = train_topic\n",
    "    assert(len(train_topic) == len(test_topic))\n",
    "    assert(set(train_topic)-set(test_topic) == set())\n",
    "    \n",
    "    topic_id_map = dict(zip(train_topic, range(len(train_topic))))\n",
    "    \n",
    "    assert isinstance(topic, list)\n",
    "    train_data_dir = [os.path.join(train_path, _) for _ in topic]\n",
    "    test_data_dir = [os.path.join(test_path, _) for _ in topic]\n",
    "    \n",
    "    train_data = [read_file(os.path.join(_, __)) for _ in train_data_dir \n",
    "                  for __ in os.listdir(_)]                               \n",
    "    test_data = [read_file(os.path.join(_, __)) for _ in test_data_dir \n",
    "                 for __ in os.listdir(_)]                               \n",
    "                                   \n",
    "    train_label = [topic_id_map[top] for top in topic \n",
    "                   for _ in range(len(os.listdir(os.path.join(train_path, top))))]\n",
    "    test_label = [topic_id_map[top] for top in topic \n",
    "                   for _ in range(len(os.listdir(os.path.join(test_path, top))))]\n",
    "    \n",
    "    print (\"train data:%d\" %len(train_data))\n",
    "    print (\"train label:%d\" %len(train_label))\n",
    "    print (\"test data:%d\" %len(test_data))\n",
    "    print (\"test label:%d\" %len(test_data))\n",
    "    return train_data, train_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:11314\n",
      "train label:11314\n",
      "test data:7532\n",
      "test label:7532\n"
     ]
    }
   ],
   "source": [
    "root_path = \"./data/20news-bydate\"\n",
    "train_data, train_label, test_data, test_label = load_data(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/stop_word.txt', 'r') as f:\n",
    "    stop_word = set([_.strip() for _ in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_cut(docs):\n",
    "    docs = [_.lower() for _ in docs]\n",
    "    docs = [list(jieba.cut(_)) for _ in docs]\n",
    "    return docs\n",
    "\n",
    "def doc_filter(docs):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        doc = filter(lambda x:x.isalnum(), doc)\n",
    "        doc = filter(lambda x:not x.isdigit(), doc)\n",
    "        doc = filter(lambda x:x not in stop_word, doc)\n",
    "        res.append(list(doc))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.656 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "train_data = doc_cut(train_data)\n",
    "train_datac = doc_filter(train_data)\n",
    "\n",
    "test_data = doc_cut(test_data)\n",
    "test_datac = doc_filter(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(train_data)\n",
    "dictionary.filter_extremes(no_below=5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build train test set for Text-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_test_set_id(docs, dictionary, length = 1024):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        vec = dictionary.doc2idx(doc)\n",
    "        vec = [_ for _ in vec if _ != -1]\n",
    "        while(len(vec) < length):\n",
    "            vec += vec\n",
    "        vec = vec[:length]\n",
    "        res.append(vec)\n",
    "    return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = build_train_test_set_id(train_datac, dictionary)\n",
    "test_set = build_train_test_set_id(test_datac, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = np.array(train_set)\n",
    "testset = np.array(test_set)\n",
    "\n",
    "trainlabel = np.array(train_label)\n",
    "testlabel = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data.jl.z']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jl.dump([trainset, trainlabel, testset, testlabel], './data/data-id.jl.z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## build train test set for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test_set_bow(docs, dictionary):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        vec = dictionary.doc2bow(doc)\n",
    "        res.append(vec)\n",
    "    return res\n",
    "\n",
    "def build_sparse_metrix(vecs, max_feature = -1):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    max_feat = 0\n",
    "    for vec_index, vec in enumerate(vecs):\n",
    "        for vec_pos, vec_val in vec:\n",
    "            if max_feature!=-1 and vec_pos < max_feature:\n",
    "                row.append(vec_index)\n",
    "                col.append(vec_pos)\n",
    "                data.append(vec_val)\n",
    "                max_feat = max_feature\n",
    "            elif max_feature == -1:\n",
    "                row.append(vec_index)\n",
    "                col.append(vec_pos)\n",
    "                data.append(vec_val)\n",
    "                max_feat = max(max_feat, vec_pos)\n",
    "                \n",
    "    sparse_matrix = csc_matrix((data, (row, col)), shape=(len(vecs), max_feat+1))        \n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep word cnt:15773\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=10)\n",
    "print (\"keep word cnt:%d\" %len(dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = build_train_test_set_bow(train_data, dictionary)\n",
    "test_set = build_train_test_set_bow(test_data, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = build_sparse_metrix(train_set, max_feature=15773)\n",
    "testset = build_sparse_metrix(test_set, max_feature=15773)\n",
    "\n",
    "trainlabel = np.array(train_label)\n",
    "testlabel = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/data-bow.jl.z']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jl.dump([trainset, trainlabel, testset, testlabel], './data/data-bow.jl.z')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
