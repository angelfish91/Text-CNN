{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib as jl\n",
    "from scipy.sparse import csc_matrix\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_data(root_path):\n",
    "    train_path = os.path.join(root_path, '20news-bydate-train')\n",
    "    test_path = os.path.join(root_path, '20news-bydate-test')\n",
    "    train_topic = os.listdir(train_path)\n",
    "    test_topic = os.listdir(test_path)\n",
    "    topic = train_topic\n",
    "    assert(len(train_topic) == len(test_topic))\n",
    "    assert(set(train_topic)-set(test_topic) == set())\n",
    "    \n",
    "    topic_id_map = dict(zip(train_topic, range(len(train_topic))))\n",
    "    \n",
    "    assert isinstance(topic, list)\n",
    "    train_data_dir = [os.path.join(train_path, _) for _ in topic]\n",
    "    test_data_dir = [os.path.join(test_path, _) for _ in topic]\n",
    "    \n",
    "    train_data = [read_file(os.path.join(_, __)) for _ in train_data_dir \n",
    "                  for __ in os.listdir(_)]                               \n",
    "    test_data = [read_file(os.path.join(_, __)) for _ in test_data_dir \n",
    "                 for __ in os.listdir(_)]                               \n",
    "                                   \n",
    "    train_label = [topic_id_map[top] for top in topic \n",
    "                   for _ in range(len(os.listdir(os.path.join(train_path, top))))]\n",
    "    test_label = [topic_id_map[top] for top in topic \n",
    "                   for _ in range(len(os.listdir(os.path.join(test_path, top))))]\n",
    "    \n",
    "    print (\"train data:%d\" %len(train_data))\n",
    "    print (\"train label:%d\" %len(train_label))\n",
    "    print (\"test data:%d\" %len(test_data))\n",
    "    print (\"test label:%d\" %len(test_data))\n",
    "    return train_data, train_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:11314\n",
      "train label:11314\n",
      "test data:7532\n",
      "test label:7532\n"
     ]
    }
   ],
   "source": [
    "root_path = \"./data/20news-bydate\"\n",
    "train_data, train_label, test_data, test_label = load_data(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/stop_word.txt', 'r') as f:\n",
    "    stop_word = set([_.strip() for _ in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_cut(docs):\n",
    "    docs = [_.lower() for _ in docs]\n",
    "    docs = [list(jieba.cut(_)) for _ in docs]\n",
    "    return docs\n",
    "\n",
    "def doc_filter(docs):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        doc = filter(lambda x:x.isalnum(), doc)\n",
    "        doc = filter(lambda x:not x.isdigit(), doc)\n",
    "        doc = filter(lambda x:x not in stop_word, doc)\n",
    "        res.append(list(doc))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.558 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "train_data = doc_cut(train_data)\n",
    "train_datac = doc_filter(train_data)\n",
    "\n",
    "test_data = doc_cut(test_data)\n",
    "test_datac = doc_filter(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(train_data)\n",
    "dictionary.filter_extremes(no_below=5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build train test set for Text-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_test_set_id(docs, dictionary, length = 1024):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        vec = dictionary.doc2idx(doc)\n",
    "        vec = [_ for _ in vec if _ != -1]\n",
    "        while(len(vec) < length):\n",
    "            vec += vec\n",
    "        vec = vec[:length]\n",
    "        res.append(vec)\n",
    "    return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = build_train_test_set_id(train_datac, dictionary)\n",
    "test_set = build_train_test_set_id(test_datac, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = np.array(train_set)\n",
    "testset = np.array(test_set)\n",
    "\n",
    "trainlabel = np.array(train_label)\n",
    "testlabel = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data.jl.z']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jl.dump([trainset, trainlabel, testset, testlabel], './data/data-id.jl.z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## build train test set for BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_test_set_bow(docs, dictionary):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        vec = dictionary.doc2bow(doc)\n",
    "        res.append(vec)\n",
    "    return res\n",
    "\n",
    "def build_sparse_metrix(vecs, max_feature = -1):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    max_feat = 0\n",
    "    for vec_index, vec in enumerate(vecs):\n",
    "        for vec_pos, vec_val in vec:\n",
    "            if max_feature!=-1 and vec_pos < max_feature:\n",
    "                row.append(vec_index)\n",
    "                col.append(vec_pos)\n",
    "                data.append(vec_val)\n",
    "                max_feat = max_feature\n",
    "            elif max_feature == -1:\n",
    "                row.append(vec_index)\n",
    "                col.append(vec_pos)\n",
    "                data.append(vec_val)\n",
    "                max_feat = max(max_feat, vec_pos)\n",
    "                \n",
    "    sparse_matrix = csc_matrix((data, (row, col)), shape=(len(vecs), max_feat+1))        \n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep word cnt:15773\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=10)\n",
    "print (\"keep word cnt:%d\" %len(dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = build_train_test_set_bow(train_data, dictionary)\n",
    "test_set = build_train_test_set_bow(test_data, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = build_sparse_metrix(train_set, max_feature=15773)\n",
    "testset = build_sparse_metrix(test_set, max_feature=15773)\n",
    "\n",
    "trainlabel = np.array(train_label)\n",
    "testlabel = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/data-bow.jl.z']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jl.dump([trainset, trainlabel, testset, testlabel], './data/data-bow.jl.z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(train_datac)\n",
    "dictionary.filter_extremes(no_below=5, keep_n=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combination(dictionary, docs, n_gram = 3, unique = True, verbose = True):\n",
    "    assert isinstance(n_gram, int) and n_gram > 0\n",
    "    token_idx_combination = list()\n",
    "    for doc in docs:\n",
    "        idx = dictionary.doc2idx(doc)\n",
    "        for index, token in enumerate(idx):\n",
    "            if index+n_gram-1 < len(idx) and len([ _ for _ in range(n_gram) if idx[index+_] == -1])==0:\n",
    "                token_idx_combination.append(\"-\".join([str(idx[index+_]) for _ in range(n_gram)]))\n",
    "    if verbose:\n",
    "        print(\"%d-gram count:%d\\tunique count:%d\" \\\n",
    "          %(n_gram, len(token_idx_combination), len(set(token_idx_combination))))\n",
    "    if unique:\n",
    "        return list(set(token_idx_combination))\n",
    "    else:\n",
    "        return token_idx_combination\n",
    "\n",
    "\n",
    "def build_train_test_set_ngrams(docs, dictionary, n_gram = 3):\n",
    "    res = []\n",
    "    for doc in docs:\n",
    "        vec = []\n",
    "        for i in range(1, n_gram+1):\n",
    "            vec += get_combination(dictionary, [doc], i, unique = False, verbose = False)\n",
    "        res.append(vec)\n",
    "    return res\n",
    "\n",
    "def build_sparse_metrix_ngrams(vecs, n_gram_list):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    n_gram_map = dict(zip(n_gram_list, range(len(n_gram_list))))\n",
    "    max_feat = len(n_gram_list)\n",
    "    for vec_index, vec in enumerate(vecs):\n",
    "        for idx_str, cnt in Counter(vec).items():\n",
    "            if idx_str in n_gram_map:\n",
    "                col.append(n_gram_map[idx_str])\n",
    "                row.append(vec_index)\n",
    "                data.append(cnt)\n",
    "    sparse_matrix = csc_matrix((data, (row, col)), shape=(len(vecs), max_feat))        \n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram count:1400109\tunique count:23465\n",
      "2-gram count:1231663\tunique count:619260\n",
      "3-gram count:1094578\tunique count:702048\n"
     ]
    }
   ],
   "source": [
    "onegram = get_combination(dictionary, train_datac, n_gram = 1)\n",
    "bigram = get_combination(dictionary, train_datac, n_gram = 2)\n",
    "trigram = get_combination(dictionary, train_datac, n_gram = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = build_train_test_set_ngrams(train_datac, dictionary, n_gram = 3)\n",
    "test_vec = build_train_test_set_ngrams(test_datac, dictionary, n_gram = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_gram_list = onegram + bigram + trigram\n",
    "testset = build_sparse_metrix_ngrams(test_vec, n_gram_list)\n",
    "trainset = build_sparse_metrix_ngrams(train_vec, n_gram_list)\n",
    "\n",
    "trainlabel = np.array(train_label)\n",
    "testlabel = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/data-ngrams.jl.z']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jl.dump([trainset, trainlabel, testset, testlabel], './data/data-ngrams.jl.z')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
