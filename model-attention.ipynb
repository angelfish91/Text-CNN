{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.nn import bidirectional_dynamic_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000, index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, seq_len):\n",
    "    return np.array([x[:seq_len - 1] + [0] * max(seq_len - len(x), 1) for x in X])\n",
    "\n",
    "def get_vocabulary_size(X):\n",
    "    return max([max(x) for x in X]) + 1  # plus the 0th word\n",
    "\n",
    "def fit_in_vocabulary(X, voc_size):\n",
    "    return [[w for w in x if w < voc_size] for x in X]\n",
    "\n",
    "def batch_generator(X, y, batch_size):\n",
    "    \"\"\"Primitive batch generator \n",
    "    \"\"\"\n",
    "    size = X.shape[0]\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    X_copy = X_copy[indices]\n",
    "    y_copy = y_copy[indices]\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i + batch_size <= size:\n",
    "            yield X_copy[i:i + batch_size], y_copy[i:i + batch_size]\n",
    "            i += batch_size\n",
    "        else:\n",
    "            i = 0\n",
    "            indices = np.arange(size)\n",
    "            np.random.shuffle(indices)\n",
    "            X_copy = X_copy[indices]\n",
    "            y_copy = y_copy[indices]\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 250)\n",
      "(25000, 250)\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 250\n",
    "\n",
    "vocabulary_size = get_vocabulary_size(X_train)\n",
    "X_test = fit_in_vocabulary(X_test, vocabulary_size)\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(inputs, attention_size, time_major=False, return_alphas=False):\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
    "\n",
    "    # Trainable parameters\n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    if not return_alphas:\n",
    "        return output\n",
    "    else:\n",
    "        return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 150\n",
    "ATTENTION_SIZE = 50\n",
    "KEEP_PROB = 0.8\n",
    "DELTA = 0.5\n",
    "\n",
    "def build_net(sess): \n",
    "\n",
    "    with tf.name_scope('Inputs'):\n",
    "        batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "        target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "        seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "        keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')\n",
    "\n",
    "    with tf.name_scope('Embedding_layer'):\n",
    "        embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), \n",
    "                                     trainable=True)\n",
    "        tf.summary.histogram('embeddings_var', embeddings_var)\n",
    "        batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)\n",
    "        print(\"%-20s\\t%s\"%(\"batch_embedded:\",batch_embedded.shape))\n",
    "\n",
    "    with tf.name_scope('Bi_RNN'):\n",
    "        rnn_outputs, _ = bidirectional_dynamic_rnn(GRUCell(HIDDEN_SIZE), \n",
    "                                                   GRUCell(HIDDEN_SIZE),\n",
    "                                                   inputs=batch_embedded, \n",
    "                                                   sequence_length=seq_len_ph, \n",
    "                                                   dtype=tf.float32)\n",
    "        tf.summary.histogram('RNN_outputs', rnn_outputs)\n",
    "        print(\"%-20s\\t%s %s\"%(\"RNN_outputs:\",rnn_outputs[0].shape, rnn_outputs[1].shape))\n",
    "    \n",
    "    with tf.name_scope('Attention_layer'):\n",
    "        attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "        drop = tf.nn.dropout(attention_output, keep_prob_ph)\n",
    "        tf.summary.histogram('alphas', alphas)\n",
    "        print(\"%-20s\\t%s\"%(\"Attention_layer output:\",attention_output.shape))\n",
    "        print(\"%-20s\\t%s\"%(\"Attention_layer alpha:\",alphas.shape))\n",
    "    \n",
    "    with tf.name_scope('Fully_connected_layer'):\n",
    "        W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  \n",
    "        b = tf.Variable(tf.constant(0., shape=[1]))\n",
    "        y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "        y_hat = tf.squeeze(y_hat)\n",
    "        tf.summary.histogram('W', W)\n",
    "        print(\"%-20s\\t%s\"%(\"Fully_connected_layer:\",y_hat.shape))\n",
    "        \n",
    "    with tf.name_scope('Metrics'):\n",
    "        # Cross-entropy loss and optimizer initialization\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "        # Accuracy metric\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    return batch_ph, target_ph, seq_len_ph, keep_prob_ph, optimizer, accuracy, loss,  merged\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, X_test, y_train, y_test, BATCH_SIZE = 256, NUM_EPOCHS = 3):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    batch_ph, target_ph, seq_len_ph, keep_prob_ph, optimizer, accuracy, loss,  merged = build_net(sess)\n",
    "    \n",
    "    train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "    test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "    train_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n",
    "\n",
    "    #session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "    session_conf = tf.ConfigProto()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "\n",
    "            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "\n",
    "            # Training\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.array([SEQUENCE_LENGTH for _ in range(len(x_batch))]) \n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keep_prob_ph: KEEP_PROB})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "\n",
    "            # Testing\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.array([SEQUENCE_LENGTH for _ in range(len(x_batch))]) \n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test\n",
    "            ))\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_embedded:     \t(?, 250, 100)\n",
      "RNN_outputs:        \t(?, 250, 150) (?, 250, 150)\n",
      "Attention_layer output:\t(?, 300)\n",
      "Attention_layer alpha:\t(?, 250)\n",
      "Fully_connected_layer:\t<unknown>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n",
      "epoch: 0\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [05:20<00:00,  3.31s/it]\n",
      "100%|██████████| 97/97 [02:46<00:00,  1.69s/it]\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.452, val_loss: 0.445, acc: 0.682, val_acc: 0.792\n",
      "epoch: 1\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [04:58<00:00,  2.97s/it]\n",
      "100%|██████████| 97/97 [02:53<00:00,  1.81s/it]\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.334, val_loss: 0.355, acc: 0.842, val_acc: 0.841\n",
      "epoch: 2\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [05:20<00:00,  3.11s/it]\n",
      "100%|██████████| 97/97 [02:54<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.241, val_loss: 0.328, acc: 0.889, val_acc: 0.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-1faf28a31b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-f7423d0acd08>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X_train, X_test, y_train, y_test, BATCH_SIZE, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtest_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
